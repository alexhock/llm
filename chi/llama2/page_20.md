```markdown
# 4 Safety

**WARNING: This section contains examples of text that may be considered rude, offensive, or upsetting.**

In this section, we dive deeper into the important topic of safety measurements and mitigations. We first discuss our safety investigations into pretraining data and pretrained models (Section 4.1). Next, we describe the process of our safety alignment (Section 4.2), explaining how we conducted safety-related annotations and utilized SFT and RLHF, and present experimental results. Then, we discuss the training we performed to further understand and improve model safety (Section 4.3). Finally, we present quantitative safety evaluations of LLaMA 2-Chat (Section 4.4). We also share a model card in the Appendix, in Table 52.

## 4.1 Safety in Pretraining

It is important to understand what is in the pretraining data to both increase transparency and to shed light on root causes of potential downstream issues, such as potential biases. This can inform what, if any, downstream mitigations to consider, and help guide appropriate model use. In this section, we analyze the pretraining data for distributions of languages, demographic representations, and toxicity. We also present the results of testing the pretrained models on existing safety benchmarks.

### Steps Taken to Pretrain Responsibly

We followed Meta’s standard privacy and legal review processes for each dataset used in training. We did not use any Meta user data in training. We excluded data from certain sites known to contain a high volume of personal information about private individuals. We made a best effort to train our models efficiently to reduce the carbon footprint of pretraining (Section 2.21). Sharing our models broadly will reduce the need for others to train similar models. No additional filtering was conducted on the datasets, to allow LLaMA 2 to be more widely usable across tasks (e.g., it can be better used for large speech classification), while avoiding the potential for the accidental demographic erasure sometimes caused by over-recoding. Importantly, this allows LLaMA 2-Chat to generate responses effectively during safety tuning with fewer examples (Welbi et al., 2021; Korkab et al., 2023; Xu et al., 2021). As a result, LLaMA 2 models should be used carefully and deployed only after significant safety tuning is applied.

### Demographic Representation: Pronouns

Bias in model generations may result from biases inherited from the training data itself. For instance, Bailey et al. (2022) shows that in massive text corpora, words representing “people” are often used in more similar contexts to words representing “them” than to words representing “women,” and Ganesh et al. (2023) demonstrates that a model’s performance on fairness metrics can be highly dependent on how the model trains on the training set's understanding of demographic groups. Within our English-language training corpus, we computed the frequencies of the most common English pronouns in Table 9. We observe that the pronouns are generally overrepresented in documents compared to 50 pronouns, echoing similar frequency differences observed in pronounual use for similarly sized model pretraining datasets (Chowdhery et al., 2022). This could mean that the model is learning less during pretraining about contexts that minimize She pronouns, and subsequently may potentially generate He pronouns at a higher rate than She pronouns.

### Demographic Representation: Identities

We also analyze the representation of different demographic groups in the pretraining data by measuring rates of usage of demographic identity terms from the HolisticBias dataset (Smith et al., 2022) as a proxy. We configure frequencies for each descriptor term in the pretraining corpus. We group descriptors into 5 areas (Religion, Gender and Sex, Nationality, Race and Ethnicity, and Sexual Orientation), and show the top 5 terms in each as shown in Table 8. In the top 5 terms, we remove a few terms such as “straight”, “white”, and “black,” because these terms have risks beyond demographic mentions (e.g., as taboo terms). We also delineate across lines, for gender as found in both Gender and Sex and Sexual Orientation. For Gender and Sex, while pronouns are mentioned in fewer documents, the term “female” is present in a larger percentage of documents. This could imply that while there is less frequent contact about She pronouns, comments about “females” are more prevalent, perhaps reflecting the differences in linguistic markers of these terms (Blodgett et al., 2021). For Sexual Orientation, the top five terms all relate to LGBTQ+ identities. For Nationality, Race and Ethnicity, and Religion, we observe a Western skew (Bhatt et al., 2022). For instance, the term “American” is mentioned in 69.4% of the references, the term “European” is more prevalent than other race and ethnicity, and “Christian” is the most represented religion followed by “Catholic” and “Jesuit”.
```