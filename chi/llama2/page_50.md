Sure! Here's the extracted content in markdown format:

```markdown
# Table 22: 
(Left) NaturalQuestions. Exact match performance. (Right) TriviaQA. Zero-shot and few-shot exact match performance on the filtered dev set. For TriviaQA, we evaluate on Wiki validation subset.

| Model      | Size | 0-shot | 1-shot | 5-shot | 64-shot | 0-shot | 1-shot | 5-shot | 64-shot |
|------------|------|--------|--------|--------|---------|--------|--------|--------|---------|
| MPT        | 7B   | 11.6   | 17.8   | 20.8   | 22.7    | 55.7   | 59.6   | 61.2   | 61.6    |
|            | 30B  | 15.8   | 23.0   | 26.6   | 29.3    | 68.0   | 71.3   | 73.3   | 73.6    |
| Falcon     | 40B  | 26.3   | 29.5   | 33.5   | 35.5    | 74.6   | 78.9   | 79.9   | 79.6    |
|            | 7B   | 16.8   | 18.7   | 22.0   | 26.1    | 63.3   | 67.4   | 70.4   | 71.0    |
|            | 13B  | 24.9   | 28.3   | 32.9   | 36.0    | 78.7   | 80.7   | 88.3   | 88.6    |
|            | 65B  | 23.8   | 31.0   | 35.0   | 39.9    | 81.7   | 84.5   | 85.9   | 86.0    |
| LLaMA 1    | 7B   | 16.4   | 22.7   | 25.7   | 29.5    | 65.8   | 68.9   | 72.1   | 73.7    |
|            | 13B  | 16.1   | 28.0   | 31.2   | 34.6    | 73.1   | 77.2   | 79.6   | 79.4    |
| LLaMA 2    | 34B  | 25.1   | 30.0   | 32.8   | 39.9    | 81.0   | 83.3   | 84.5   | 84.6    |
|            | 70B  | 25.3   | 33.0   | 39.5   | 43.4    | 82.4   | 85.0   | 87.6   | 87.5    |

---

# Table 23: Comparison to open-source models on reading comprehension (SQUAD and QUAC).

| Model      | Size | 0-shot | 1-shot | 4-shot | 5-shot | 0-shot | 1-shot | 0-shot | 1-shot |
|------------|------|--------|--------|--------|--------|--------|--------|--------|--------|
| MPT        | 7B   | 59.5   | 62.8   | 62.6   | 62.7   | 38.0   | 37.7   |        |        |
|            | 30B  | 74.7   | 74.2   | 74.2   | 74.0   | 40.4   | 41.1   |        |        |
| Falcon     | 7B   | 16.4   | 16.0   | 16.9   | 17.5   | 24.0   | 18.8   |        |        |
|            | 40B  | 72.9   | 73.1   | 71.7   | 71.0   | 41.2   | 43.3   |        |        |
| LLaMA 1    | 7B   | 60.0   | 62.3   | 63.3   | 62.8   | 39.2   | 42.3   |        |        |
|            | 13B  | 68.9   | 68.4   | 66.4   | 66.7   | 39.9   | 36.5   |        |        |
|            | 33B  | 75.5   | 70.0   | 76.3   | 75.6   | 44.1   | 30.3   |        |        |
|            | 65B  | 79.4   | 80.0   | 78.3   | 77.9   | 41.1   | 39.8   |        |        |

---

# Table 24: Comparison to open source models on AGI Eval (English)

| Model      | Size | Avg  | AQuA-RAT | LogQA | LSAT-AR | LSAT-FR | LSAT-RC | SaTEn (w/o Pgs) | SaTmath |
|------------|------|------|----------|-------|---------|---------|---------|-----------------|---------|
| MPT        | 78   | 27.6 | 23.8     | 18.7  | 21.2    | 20.8    | 25.2    | 23.6            | 23.6    |
| MPT        | 30B  | 31.8 | 26.5     | 23.7  | 29.3    | 37.1    | 37.9    | 26.3            | 27.7    |
| Falcon     | 40B  | 21.7 | 28.3     | 16.1  | 17.3    | 20.4    | 24.8    | 26.3            | 26.4    |
|            | 80B  | 25.6 | 19.6     | 10.2  | 42.4    | 54.7    | 58.7    |                   |         |
| LLaMA 1    | 7B   | 23.9 | 19.9     | 24.5  | 26.5    | 35.1    | 29.3    |                   |         |
|            | 13B  | 31.8 | 31.0     | 23.2  | 13.6    | 32.5    | 34.4    | 27.1            | 28.2    |
|            | 34B  | 39.1 | 37.3     | 18.7  | 36.0    | 35.9    | 24.5    | 41.7            | 28.7    |
|            | 65B  | 42.6 | 21.6     | 21.3  | 42.0    | 56.7    | 63.0    | 43.8            | 41.8    |
| LLaMA 2    | 13B  | 34.1 | 21.7     | 23.0  | 21.3    | 41.0    | 54.6    | 30.7            | 27.3    |
|            | 34B  | 34.3 | 19.3     | 20.3  | 21.5    | 42.1    | 27.2    | 32.7            | 32.7    |
|            | 70B  | 54.2 | 42.5     | 25.7  | 70.2    | 76.6    | 58.9    | 39.4            | 41.8    |
```
