
# References

1. Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. Discriminative adversarial search for abstractive summarization. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of *Proceedings of Machine Learning Research*, pages 8555–8564. PMLR, 13–18 Jul 2020. URL: https://proceedings.mlr.press/v119/scialom20a.html.

2. Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. Coldgans: Taming language models with cautious sampling strategies. *Advances in Neural Information Processing Systems*, 33:18978–18989, 2020.

3. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units, 2016.

4. Uri Shaham, Eald Segal, Moar Igy, Aria Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized ComPResS over long language sequences. In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages 1207–1221, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL: https://aclanthology.org/2022.emnlp-main.823.

5. Noam Shazeer. Fast transformer decoding: One write-head is all you need, 2019.

6. Noam Shazeer. GLUE variants improve transformer, 2020.

7. Mohammad Shoeybi, Mostafa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-t: Training multi-billion parameter language models using model parallelism, 2019.

8. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolae Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget. *arXiv preprint arXiv:2305.14793*, 2023.

9. Eric Michael Smith and Adina Williams. Hi, my name is martha: Using names to measure and mitigate bias in generative dialogue models. *arXiv preprint arXiv:2109.03300*, 2021.

10. Eric Michael Smith, Melissa Hall, Melanke Kullman, Eleonora Prensi, and Adina Williams. “I’m sorry to hear that!”: Finding new biases in language models with a holistic descriptor dataset. In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages 9180–9211, 2022.

11. Irene Loaiza, Zeerak Shauqat, William Agnew, Lama Ahmad, Dylan Bakker, Su Lin Blodgett, Hal Daumé III, Jesse Dodge, Elle Evans, Sara Hooker, et al. Evaluating the social impact of generative AI systems in society and society. *arXiv preprint arXiv:2306.54909*, 2023.

12. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Zorio Amedeo, and Paul Christian. Learning to summarize from human feedback in NeurIPS, 2020.

13. Jianlin, Su Yu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Reformer: Enhanced transformer with rotary position embedding, 2020.

14. Mirac Suzgun, Nathan Scales, Nathaniel Shafit, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Akanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chair-of-strings can solve them. *arXiv preprint arXiv:2210.02961*, 2022.

15. Gabriel Synnaeve, Jonas Cahring, Zeming Lin, Daniel Haiza, Nicolas Luthiger, Danielle Rothermel, Vegard Melia, D.J. Nicolas Carlgr, Laura Gustafson, et al. Growing up together: Structured exploration for large-scale pretraining, 2019.

16. Yarden Tal, Inbal Magar, and Roy Schwartz. Fewer errors, but more bias: The effect of model size on gender bias. In *Proceedings of the 8th Workshop on Gender Bias in Natural Language Processing (GeBNLP)*, pages 112–120, Seattle, Washington, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.gebnlp-1.13. URL: https://aclanthology.org/2022.gebnlp-1.13.

17. Alan Turing, Jonathan Hertz, Nicholas Louwere, and Jonathan Berant. Commonsense: A question answering challenge targeting commonsense knowledge. *arXiv preprint arXiv:1811.00373*, 2018.

18. Rohan Tauri, Ishaan Guliaarani, Tianyi Zhang, Yanzh Wang, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsuhiro B. Hashimoto. Stanford Alpaca: An instruction-following Llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

19. Ross Taylor, Marcin Kardaś, Guilleu Ccuruill, Thomas Scialom, Anthony Harshborn, Elvis Savaria, Andrew Poulton, Viktor Kerkes, and Robert Stojnic. Galactica: A large language model for science. *arXiv preprint arXiv:2211.09805*, 2022.
