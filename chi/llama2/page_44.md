
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naaman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Arnaud Bodin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, John Atten, Ali A. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017.

Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Time Ewals, Róbert Gehrig, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.

Yizhong Wang, Yeqing Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hanzi Wang. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.

Michael Webb. The impact of artificial intelligence on the labor market. Available at SSRN 3482750, 2019.

Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021.
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=GfzZc0dcp.

Jason Wei, Maarten Bosma, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022.

Laura Weidinger, John Mellor, Marthebit Zeahl, Corrin, Jonathan Dathathil, John Mello, Lisa Anne Hendricks, Mia Glase, Borja Balle, Akoto Kasirzadeh, et al. Ethical and social risks of harm from language models. arXiv preprint arXiv:2212.03592, 2022.

Johannes Rüsch, Amelia Glaese, Jonathan Uesato, Sumant Dathathil, John Mello, Lisa Anne Hendricks, Kristy Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Hsu. Challenges in identifying language models, 2021.

Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Armandi, Kiwon Maeng, Gloria Chang, Hong Aqa, Jinsih Huang, Charles Bai, et al. Sustainable AI: Environmental implications, challenges, and recommendations. Proceedings of Machine Learning and Systems, 7:495–813, 2022.

Jing Xu, D. J. Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. Recipes for safety in open-domain chatbots, 2021.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.

Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Kossinets, and Yejin Choi. Defending against neural fake news. Advances in neural information processing systems, 32, 2019.

Biao Zhang and Rico Sennrich. Root mean square layer normalization, 2019.

Susan Zhang, Stephen Roller, Naaman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Devan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.00572, 2022.

Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chi Huang, Min Xu, Less Wright, Hamid Shojaeizadeh, Myke Ott, Sam Shleifer, Alban Desmaison, Can Bilgilu, Benjamin Geer, Yeqin Chauhan, Xindong Hua, and Shen Li. Pytorch biggraph: Experiences on scaling fully sharded data parallel, 2023.

Yuanqing Zhou, Andrei Ion Muresan, Zewang Han, Keiran Paster, Silvio Tulis, Harsimran Chani, and Jimmy Hsu. Large language models are human-centric benchmark engineers. In The Eleventh International Conference on Learning Representations, 2022.
