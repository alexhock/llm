
A.4.7  Description of Automatic Safety Benchmarks

In this section, we provide a detailed description about the automatic safety benchmarks we use for evaluation from the perspectives of truthfulness, toxicity, and bias.

**Truthfulness.** To understand the ability of LLMs to generate reliable outputs that agree with factuality and common sense, we employ TruthfulQA (Lin et al., 2021), used for LLM hallucinations to measure whether a language model is truthful in generating answers to questions while being informative at the same time. The TruthfulQA benchmark consists of 817 questions, distributed across 38 categories, including but not limited to health, finance, law, and politics (Lin et al., 2021). The questions are designed in a way that even humans might answer incorrectly because of an unfounded belief or misconception. Following Lin et al. (2021) we use GPT-3-based metrics, which have been shown to have robust performance in predicting human judgments. Specifically, a fine-tuned GPT-3 model³, i.e. a “GPT-Qjudge”, is used to predict the truthfulness and informativeness of the generated outputs from LLMs. For the QA prompt, we adopt a few-shot prompt containing 6 random QA pairs with the formats following InstructGPT (Ouyang et al., 2022). We report the percentage of generations that are both truthful and informative, as well as the percentage that are either truthful or informative.

**Toxicity.** To measure the degree of generation of toxic language and hate speech across different groups, we use ToxicGen (Hartvigsen et al., 2022), a dataset that contains implicitly toxic and benign sentences mentioning 13 minority groups. We adopt a revised version of the dataset from Hosseini et al. (2023) that reduces noise by filtering out prompts for which annotations disagree on the target demographic group. We then use the default ToxicBert classifier tuned on RoBERTa (Liu et al., 2019) to measure the toxicity of generations of the LLMs.

**Bias.** To study the sentiment in model generations that may vary with demographic attributes, we choose BOLD (Dhamala et al., 2012), a large-scale bias benchmark that comprises 23,679 English Wikipedia prompts spanning five domains of race, gender, religion, political ideology, and profession, with 34 different subgroups⁴. We conduct a sentiment analysis using the Valence Aware Dictionary and Sentiment Reasoner (VADER) (Hutto and Gilbert, 2014) to evaluate the sentiments discovered by the combination of prompt prefix and model generation. VADER produces a sentiment score between -1 and 1. A positive (negative) score indicates a positive (negative) sentiment towards the population mentioned in the prompt, and a score closer to 0 indicates a neutral sentiment.

A.4.8  Automatic Safety Benchmark Evaluation Results

Fine-grained Analysis of Toxicity, Truthfulness, and Bias. Here we provide in-depth analyses to understand the safety of model generations from the perspectives of toxicity, truthfulness, and bias.

- **Truthfulness.** Table 44 presents evaluation results of TruthfulQA for the percentage of truthfulness, percentage of informativeness, and percentage of both truthfulness and informativeness across generations. Most of the models show a >90% informativeness in the model generations. However, the truthfulness percentage is relatively low for pretrained models, around 30% to 40% for Falcon, MPT, and the 7B LLaMA 1. This percentage increases for pretrained LLaMA 1 and LLaMA 2 with a larger size. After instruction fine-tuning, both 7B and 13B LLaMA 2-Chat improved about 20% in truthfulness, 30B LLaMA 2-Chat improved about 24%, and 70B LLaMA 2-Chat improved about 14% compared to their pretrained versions.

- **Toxicity.** Table 45 shows that Mexicans, Latinos, and women tend to be the top three demographic groups with the highest percentages of toxic generations given ToxicGen prompts for the pretrained generations. Thanks to instruction fine-tuning, fine-tuned LLaMA 2-Chat models of all sizes show an relatively zero percentage of toxic model generations, and hence their results are not presented here.

- **Bias.** Tables 46, 47, 48, and 49 present the distribution of sentiment scores across different demographic groups under the domains of race, gender, religious ideology, political ideology, and profession. Overall, we observe positive sentiment scores for each domain in the BOLD dataset for its use of "informative".
