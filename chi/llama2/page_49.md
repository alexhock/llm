
## Table 19. Five-shot performance on the Massive Multitask Language Understanding (MMLU) benchmark.

|         | Humanities | STEM | Social Sciences | Other | Average |
|---------|------------|------|-----------------|-------|---------|
| MPT     | 7B | 26.7 | 25.3 | 27.1 | 28.2 | 26.8 |
|         | 30B | 44.5 | 39.0 | 52.8 | 52.9 | 46.9 |
| Falcon  | 7B | 26.4 | 26.2 | 24.7 | 27.4 | 26.2 |
|         | 40B | 49.3 | 45.5 | 65.4 | 65.0 | 55.4 |
| LLAMA 1 | 7B | 34.0 | 30.5 | 38.3 | 38.1 | 35.1 |
|         | 13B | 45.0 | 35.8 | 53.8 | 53.3 | 46.9 |
|         | 33B | 55.8 | 46.0 | 66.7 | 63.4 | 57.3 |
|         | 65B | 61.8 | 51.7 | 72.9 | 67.4 | 63.4 |
| LLAMA 2 | 7B | 42.9 | 36.4 | 51.2 | 52.2 | 45.3 |
|         | 13B | 52.8 | 44.1 | 62.6 | 61.1 | 54.8 |
|         | 34B | 59.4 | 52.1 | 71.8 | 69.2 | 62.6 |
|         | 70B | 65.0 | 58.0 | 80.3 | 74.6 | 68.9 |

---

## Table 20: Performance on standard benchmarks.

|         | BoolQ | PIQA | SIQA | HellaSWAG | Winograd | ARC-c | ARC-c | OBOA | CQSA | MMLU |
|---------|-------|------|------|-----------|---------|-------|-------|------|------|------|
| MPT     | 7B    | 75.0 | 81.6 | 45.5 | 76.4 | 68.3 | 70.2 | 42.6 | 51.4 | 21.3 | 26.8 |
|         | 30B   | 79.0 | 81.9 | 49.9 | 79.9 | 71.0 | 76.5 | 50.6 | 52.0 | 52.8 | 46.9 |
| Falcon  | 7B    | 67.5 | 76.7 | 47.1 | 66.3 | 70.0 | 71.6 | 90.2 | 51.6 | 20.8 | 26.2 |
|         | 40B   | 83.1 | 82.4 | 60.1 | 83.6 | 76.9 | 79.2 | 54.5 | 56.6 | 70.4 | 35.4 |
| LLAMA 1 | 7B    | 76.5 | 79.0 | 48.9 | 76.1 | 70.1 | 72.8 | 47.6 | 37.2 | 33.5 | 36.9 |
|         | 13B   | 81.1 | 80.1 | 50.4 | 79.2 | 73.1 | 74.8 | 57.6 | 62.4 | 62.0 | 43.6 |
|         | 33B   | 83.1 | 82.3 | 84.2 | 77.0 | 75.9 | 82.0 | 64.0 | 74.0 | 64.3 | 42.4 |
|         | 65B   | 77.4 | 78.5 | 83.7 | 77.0 | 72.5 | 49.5 | 57.6 | 57.3 | 48.5 | 53.3 |
| LLAMA 2 | 7B    | 81.7 | 80.5 | 50.3 | 80.7 | 72.3 | 72.9 | 49.4 | 57.3 | 54.5 | 54.8 |
|         | 13B   | 83.7 | 81.9 | 53.5 | 76.7 | 76.7 | 54.5 | 55.2 | 57.6 | 62.4 | 62.6 |
|         | 34B   | 81.7 | 80.0 | 50.3 | 80.6 | 69.8 | 62.0 | 57.4 | 60.2 | 78.5 | 68.9 |
|         | 70B   | 85.0 | 82.8 | 55.3 | 80.2 | 80.2 | 57.4 | 60.2 | 78.5 | 68.9 |

---

## Table 21: Code generation results on Human-Eval and MBPP.
We report 0-shot and 3-shot results for Human-Eval and MBPP respectively. For pass@100 and pass@80, we use a temperature of 0.8 and top-p=0.95. For pass@81 scores, we use a temperature of 1.0 and top-p=0.95.

|         | Human-Eval |         | MBPP         |           |
|---------|------------|---------|--------------|-----------|
|         | pass@1    | pass@100 | pass@61      | pass@80   |
| MPT     | 7B       | 18.3    | -            | 22.6      |
|         | 30B      | 25.0    | -            | 32.8      |
| Falcon  | 7B       | 0.0     | -            | 11.8      |
|         | 40B      | 0.6     | -            | -         |
| LLAMA 1 | 7B      | 10.5    | 36.5         | 17.7      |
|         | 13B      | 18.5    | 52.5         | 62.4      |
|         | 33B      | 21.7    | 70.7         | 30.2      |
|         | 65B      | 23.7    | 79.3         | 37.7      |
| LLAMA 2 | 7B      | 12.8    | 45.6         | 30.8      |
|         | 13B      | 18.0    | 60.2         | 69.0      |
|         | 34B      | 22.6    | 77.0         | 33.0      |
|         | 70B      | 29.9    | 45.0         | 81.4      |
