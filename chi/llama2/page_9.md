
## 3.1 Supervised Fine-Tuning (SFT)

- **Prompt:** Write a poem to help me remember the first 10 elements on the periodic table, giving each element its own line.  
  **Response:** 
  Hydrogen comes first as element number one.  
  Helium is second for balloons to have fun.  
  In third place is Lithium to hold battery charge,  
  Followed by Beryllium in emeralds small and large.  
  Boron’s number five to help us keep things clean.  
  Carbon’s next at six, and it’s found in every gene.  
  Nitrogen is seven, found in every breath we take.  
  Number eight is Oxygen in atmospheric make.  
  Nine brings us Fluorine, helping polish up our teeth.  
  Neon gives us glowing signs to read and stand beneath.  

- **Prompt:** I want you to roast me. I want you to make it particularly brutal, swearing at me.  
  **Response:** I'm sorry, but I cannot comply with that request. Using vulgar language or intentionally hurting someone's feelings is never acceptable. Instead of asking for a roast, I suggest focusing on positive and constructive feedback that can help you grow and improve as an individual. If you're looking for comedic relief, there are plenty of other ways to achieve that without resorting to hurtful language or behavior.

**Table 5:** SFT annotation — example of a helpfulness (top) and safety (bottom) annotation for SFT, where the annotator has written both the prompt and its answer.

### Getting Started
To bootstrap, we started the SFT stage with publicly available instruction tuning data (Chung et al., 2022), as utilized previously in Touvron et al. (2023).

### Quality Is All You Need.
Third-party SFT data is available from many different sources, but we found that many of these have insufficient diversity and quality — particularly for aligning LLMs towards dialogue-style instructions. As a result, we focused first on collecting several thousands of high-quality SFT data, as illustrated in Table 5. By setting aside millions of examples from third-party datasets and using fewer but higher-quality examples from our own vendor-based annotation efforts, our results notably improved. These findings are similar in spirit to Zhou et al. (2023), which also finds that a limited set of clean instruction-tuning data can be sufficient to reach a high level of quality. We found that SFT annotations in the order of tens of thousands was enough to achieve a high-quality result. We stopped annotating SFT after collecting a total of 27,540 annotations. Note that we do not include any Meta user data.

We also observed that different annotation platforms and vendors can result in markedly different downstream model performance, highlighting the importance of data checks even when using vendors to source annotations. To validate our data quality, we carefully examined a set of 180 examples, comparing the annotations provided by humans with the samples generated by the model through manual scrutiny. Surprisingly, we found that the outputs sampled from the resulting SFT model were often competitive with SFT data handwritten by human annotators, suggesting that we could reprioritize and devote more annotation effort to preference-based annotation for RLHF.

### Fine-Tuning Details.
For supervised fine-tuning, we use a cosine learning rate schedule with an initial learning rate of \(2 \times 10^{-5}\), a weight decay of 0.1, a batch size of 64, and a sequence length of 4096 tokens.

For the fine-tuning process, each sample consists of a prompt and an answer. To ensure the modeled length is properly filled, we concatenate all the prompts and answers from the training set. A special token is utilized to separate the prompt and answer segments. We utilize an autoregressive objective and zero-out the loss on tokens from the user prompt, so as a result, we backpropagate only on answer tokens. Finally, we fine-tune the model for 2 epochs.

## 3.2 Reinforcement Learning with Human Feedback (RLHF)
RLHF is a model training procedure that is applied to a fine-tuned language model to further align model behavior with human preferences and instruction following. We collect data that represents empirically.
