```markdown
# 3.2.1 Human Preference Data Collection

Next, we collect human preference data for reward modeling. We chose a binary comparison protocol over other schemes, mainly because it enables us to maximize the diversity of collected prompts. Still, other strategies are worth considering, which we leave for future work.

Our annotation procedure proceeds as follows. We ask annotators to first write a prompt, then choose two responses to a given prompt are sampled from two different model variants, and varying the temperature hyper-parameter. In addition to giving participants a forced choice, we also ask annotators to label the degree to which they prefer their chosen response over the alternative: either this choice is significantly better, better, slightly better, or negligibly better/worse.

For our collection of preference annotations, we focus on helpfulness and safety. Helpfulness refers to how well **LLAMA 2-CHAT** responses fulfill users' requests and provide requested information; safety refers to whether **LLAMA 2-CHAT**'s responses are unsafe, e.g., "giving detailed instructions on making a bomb" could be considered helpful but is unsafe according to our safety guidelines. Separating the two allows us to apply specific guidelines to each and better guide annotators; for example, our safety annotations provide instructions to focus on adversarial prompts, and other guidance.

Apart from differences in annotation guidelines, we additionally collect a safety boundary regarding safety. This additional information bins model responses into one of three categories: 1) the preferred response is safe and the other response is not, 2) both responses are safe, and 3) both responses are unsafe, with 18%, 42%, and 35% of the safety dataset falling into each bin, respectively. We do not include any examples where the chosen response was unsafe and the other response safe, as we believe safer responses will also be better preferred by humans. Safety guidelines and more detailed information regarding safety annotations can be found in Section 4.2.1.

Human annotations were collected in batches on a weekly basis. As we collected more preference data, our reward modeling improved, and we were able to train progressively better versions of **LLAMA 2-CHAT** (see the results in Section 5, Figure 20). **LLAMA 2-CHAT** improvement also shifted the model's data distribution. Since reward model accuracy can quickly degrade if not exposed to its own sample distribution, i.e., from hyper-specialization (Cialdini et al., 2002), it is important before a new **LLAMA 2-CHAT** iteration to retrieve new preference data using the latest **LLAMA 2-CHAT** iterations. This step helps keep the reward model on-distribution and maintain an accurate reward for the latest model.

In Table 6, we report the statistics of reward modeling data that we collected over time and present them against multiple open-source preference datasets including Anthropic Helpful and Harmless (Bai et al., 2022), OpenAI Summarize (Steinhardt et al., 2022), OpenAI WebGPT (Nisan et al., 2022), Self-Teach (Shackfield et al., 2023), Stanford Human Preferences (Ethayarajh et al., 2022), and Synthetic GPT (Havriluk). We collected a large dataset of over 1 million binary comparisons based on humans applying our specified guidelines, which we refer to as a Meta reward modeling data. Note that the number of tokens in prompts and answers differs depending on the task domain. Summarization and online forum data generally have longer prompts, while dialogue-style prompts are usually shorter. Compared to existing open-source datasets, our preference data features more conversation turns, and are longer, on average.

# 3.2.2 Reward Modeling

The reward model takes a model response and its corresponding prompt (including contexts from previous turns) as input and outputs a scalar score to indicate the quality (e.g., helpfulness and safety) of the model's generation. Leveraging such response scores as rewards, we can optimize **LLAMA 2-CHAT** during RLHF to better human preference alignment and improved helpfulness and safety.

Others have found that helpfulness and safety sometimes trade off (Bai et al., 2022), which can make it challenging for a single reward model to perform well on both. To address this, we train two separate reward models, one optimized for helpfulness (referred to as **Helpfulness RM**) and another for safety (**Safety RM**).

We initialize our reward models from pretrained chat model checkpoints, as it ensures that both models benefit from knowledge acquired in pretraining. In short, the reward model "knows" what the chat model is generating.
```