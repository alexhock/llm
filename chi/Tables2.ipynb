{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2095ec2c-4927-41da-b3be-8f46e1142672",
      "metadata": {
        "id": "2095ec2c-4927-41da-b3be-8f46e1142672"
      },
      "outputs": [],
      "source": [
        "# extract tables and rag llamaindex"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six markdownify"
      ],
      "metadata": {
        "id": "2-AmG_yYe0sX",
        "outputId": "93bf65db-af7d-4bab-83fb-48507f3d5116",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2-AmG_yYe0sX",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20240706)\n",
            "Collecting markdownify\n",
            "  Downloading markdownify-0.13.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (42.0.8)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.9 in /usr/local/lib/python3.10/dist-packages (from markdownify) (4.12.3)\n",
            "Requirement already satisfied: six<2,>=1.15 in /usr/local/lib/python3.10/dist-packages (from markdownify) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5,>=4.9->markdownify) (2.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading markdownify-0.13.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: markdownify\n",
            "Successfully installed markdownify-0.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.high_level import extract_text_to_fp\n",
        "from markdownify import markdownify as md\n",
        "import io\n",
        "\n",
        "def pdf_to_html(pdf_path, html_path):\n",
        "    # Initialize a StringIO to capture the HTML output\n",
        "    output_string = io.StringIO()\n",
        "\n",
        "    # Extract text and layout analysis\n",
        "    with open(pdf_path, 'rb') as f:\n",
        "      extract_text_to_fp(f, output_string, laparams=LAParams(), output_type='html', codec=None)\n",
        "\n",
        "      # Save the HTML content to a file\n",
        "      html_content = output_string.getvalue()\n",
        "      with open(html_path, 'w', encoding='utf-8') as html_file:\n",
        "        html_file.write(html_content)\n",
        "\n",
        "        print(f\"HTML file saved to {html_path}\")\n",
        "\n",
        "def html2md(htmlpath, mdpath):\n",
        "  with open(htmlpath, 'r') as f:\n",
        "    html = f.read()\n",
        "\n",
        "  with open(mdpath, 'w') as f:\n",
        "    mdtext=md(html)\n",
        "    f.write(mdtext)\n",
        "\n",
        "# Example usage\n",
        "pdf_path = 'llama2.pdf'\n",
        "html_path = 'output.html'\n",
        "pdf_to_html(pdf_path, html_path)\n",
        "html2md(html_path, 'output.md')"
      ],
      "metadata": {
        "id": "TPQpuf8XetPy",
        "outputId": "112e6c6e-7569-4228-f979-ece545a81752",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TPQpuf8XetPy",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTML file saved to output.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVEkW1kaesy2"
      },
      "id": "uVEkW1kaesy2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "184713a0-7ff1-443f-a61a-e599e6ed1b76",
      "metadata": {
        "id": "184713a0-7ff1-443f-a61a-e599e6ed1b76",
        "outputId": "1e3534f2-c7de-4224-f330-eb41025ca76c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-08 09:12:28--  https://arxiv.org/pdf/2307.09288.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.131.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2307.09288 [following]\n",
            "--2024-08-08 09:12:28--  http://arxiv.org/pdf/2307.09288\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13661300 (13M) [application/pdf]\n",
            "Saving to: ‘llama2.pdf’\n",
            "\n",
            "llama2.pdf          100%[===================>]  13.03M  86.1MB/s    in 0.2s    \n",
            "\n",
            "2024-08-08 09:12:28 (86.1 MB/s) - ‘llama2.pdf’ saved [13661300/13661300]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"llama2.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7399db6-e9d9-4599-9ba2-62040b0ca3bc",
      "metadata": {
        "id": "e7399db6-e9d9-4599-9ba2-62040b0ca3bc"
      },
      "outputs": [],
      "source": [
        "pdf_file = \"llama2.pdf\"\n",
        "\n",
        "# Split the base name and extension\n",
        "output_directory_path, _ = os.path.splitext(pdf_file)\n",
        "\n",
        "if not os.path.exists(output_directory_path):\n",
        "    os.makedirs(output_directory_path)\n",
        "\n",
        "# Open the PDF file\n",
        "pdf_document = fitz.open(pdf_file)\n",
        "\n",
        "# Iterate through each page and convert to an image\n",
        "for page_number in range(pdf_document.page_count):\n",
        "    # Get the page\n",
        "    page = pdf_document[page_number]\n",
        "\n",
        "    # Convert the page to an image\n",
        "    pix = page.get_pixmap()\n",
        "\n",
        "    # Create a Pillow Image object from the pixmap\n",
        "    image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "\n",
        "    # Save the image\n",
        "    image.save(f\"./{output_directory_path}/page_{page_number + 1}.png\")\n",
        "\n",
        "# Close the PDF file\n",
        "pdf_document.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13d7f70f-f81d-4e0f-bbe2-b47a5cf6b649",
      "metadata": {
        "id": "13d7f70f-f81d-4e0f-bbe2-b47a5cf6b649"
      },
      "outputs": [],
      "source": [
        "https://docs.llamaindex.ai/en/v0.9.48/examples/multi_modal/multi_modal_pdf_tables.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14a5030b-2e4c-4d8a-9a8c-b730416fe46a",
      "metadata": {
        "id": "14a5030b-2e4c-4d8a-9a8c-b730416fe46a"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index qdrant_client pyMuPDF tools frontend git+https://github.com/openai/CLIP.git easyocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e1c0a2e-ffd5-409d-ae79-f473db3a6430",
      "metadata": {
        "id": "3e1c0a2e-ffd5-409d-ae79-f473db3a6430"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import Patch\n",
        "import io\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "from transformers import AutoModelForObjectDetection\n",
        "import torch\n",
        "import openai\n",
        "import os\n",
        "import fitz\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "OPENAI_API_TOKEN = \"sk-<your-openai-api-token>\"\n",
        "openai.api_key = OPENAI_API_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00b38eb7-eac9-44b5-876a-aebb83d9bebc",
      "metadata": {
        "id": "00b38eb7-eac9-44b5-876a-aebb83d9bebc"
      },
      "outputs": [],
      "source": [
        "class MaxResize(object):\n",
        "    def __init__(self, max_size=800):\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def __call__(self, image):\n",
        "        width, height = image.size\n",
        "        current_max_size = max(width, height)\n",
        "        scale = self.max_size / current_max_size\n",
        "        resized_image = image.resize(\n",
        "            (int(round(scale * width)), int(round(scale * height)))\n",
        "        )\n",
        "\n",
        "        return resized_image\n",
        "\n",
        "\n",
        "detection_transform = transforms.Compose(\n",
        "    [\n",
        "        MaxResize(800),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "structure_transform = transforms.Compose(\n",
        "    [\n",
        "        MaxResize(1000),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# load table detection model\n",
        "# processor = TableTransformerImageProcessor(max_size=800)\n",
        "model = AutoModelForObjectDetection.from_pretrained(\n",
        "    \"microsoft/table-transformer-detection\", revision=\"no_timm\"\n",
        ").to(device)\n",
        "\n",
        "# load table structure recognition model\n",
        "# structure_processor = TableTransformerImageProcessor(max_size=1000)\n",
        "structure_model = AutoModelForObjectDetection.from_pretrained(\n",
        "    \"microsoft/table-transformer-structure-recognition-v1.1-all\"\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# for output bounding box post-processing\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(-1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def rescale_bboxes(out_bbox, size):\n",
        "    width, height = size\n",
        "    boxes = box_cxcywh_to_xyxy(out_bbox)\n",
        "    boxes = boxes * torch.tensor(\n",
        "        [width, height, width, height], dtype=torch.float32\n",
        "    )\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def outputs_to_objects(outputs, img_size, id2label):\n",
        "    m = outputs.logits.softmax(-1).max(-1)\n",
        "    pred_labels = list(m.indices.detach().cpu().numpy())[0]\n",
        "    pred_scores = list(m.values.detach().cpu().numpy())[0]\n",
        "    pred_bboxes = outputs[\"pred_boxes\"].detach().cpu()[0]\n",
        "    pred_bboxes = [\n",
        "        elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)\n",
        "    ]\n",
        "\n",
        "    objects = []\n",
        "    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):\n",
        "        class_label = id2label[int(label)]\n",
        "        if not class_label == \"no object\":\n",
        "            objects.append(\n",
        "                {\n",
        "                    \"label\": class_label,\n",
        "                    \"score\": float(score),\n",
        "                    \"bbox\": [float(elem) for elem in bbox],\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return objects\n",
        "\n",
        "def detect_and_crop_save_table(\n",
        "    file_path, cropped_table_directory=\"./table_images/\"\n",
        "):\n",
        "    image = Image.open(file_path)\n",
        "\n",
        "    filename, _ = os.path.splitext(file_path.split(\"/\")[-1])\n",
        "\n",
        "    if not os.path.exists(cropped_table_directory):\n",
        "        os.makedirs(cropped_table_directory)\n",
        "\n",
        "    # prepare image for the model\n",
        "    # pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
        "    pixel_values = detection_transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(pixel_values)\n",
        "\n",
        "    # postprocess to get detected tables\n",
        "    id2label = model.config.id2label\n",
        "    id2label[len(model.config.id2label)] = \"no object\"\n",
        "    detected_tables = outputs_to_objects(outputs, image.size, id2label)\n",
        "\n",
        "    print(f\"number of tables detected {len(detected_tables)}\")\n",
        "\n",
        "    for idx in range(len(detected_tables)):\n",
        "        #   # crop detected table out of image\n",
        "        cropped_table = image.crop(detected_tables[idx][\"bbox\"])\n",
        "        cropped_table.save(f\"./{cropped_table_directory}/{filename}_{idx}.png\")\n",
        "\n",
        "def plot_images(image_paths):\n",
        "    images_shown = 0\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    for img_path in image_paths:\n",
        "        if os.path.isfile(img_path):\n",
        "            image = Image.open(img_path)\n",
        "\n",
        "            plt.subplot(2, 3, images_shown + 1)\n",
        "            plt.imshow(image)\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "\n",
        "            images_shown += 1\n",
        "            if images_shown >= 9:\n",
        "                break\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a090fe8-a01f-4303-a78d-8d5847f53e3b",
      "metadata": {
        "id": "8a090fe8-a01f-4303-a78d-8d5847f53e3b"
      },
      "outputs": [],
      "source": [
        "for file_path in retrieved_images:\n",
        "    detect_and_crop_save_table(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62773545-2acc-4c6f-84b0-753782f2ad6b",
      "metadata": {
        "id": "62773545-2acc-4c6f-84b0-753782f2ad6b"
      },
      "outputs": [],
      "source": [
        "# Read the cropped tables\n",
        "image_documents = SimpleDirectoryReader(\"./table_images/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a25fe2e1-af75-43c7-9ee3-2dd4964c71cc",
      "metadata": {
        "id": "a25fe2e1-af75-43c7-9ee3-2dd4964c71cc"
      },
      "outputs": [],
      "source": [
        "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
        "from llama_index import SimpleDirectoryReader\n",
        "\n",
        "# put your local directore here\n",
        "#documents_images_v2 = SimpleDirectoryReader(\"./llama2/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfa91b8d-f8b6-437f-bf9b-4b7cd15162cd",
      "metadata": {
        "id": "bfa91b8d-f8b6-437f-bf9b-4b7cd15162cd"
      },
      "outputs": [],
      "source": [
        "openai_mm_llm = OpenAIMultiModal(\n",
        "    model=\"gpt-4-vision-preview\", api_key=OPENAI_API_TOKEN, max_new_tokens=1500\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "468cff40-d06a-42e5-ba69-4810e30513f3",
      "metadata": {
        "id": "468cff40-d06a-42e5-ba69-4810e30513f3"
      },
      "outputs": [],
      "source": [
        "response = openai_mm_llm.complete(\n",
        "    prompt=\"Compare llama2 with llama1?\",\n",
        "    image_documents=image_documents,\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3cf617f-448b-41ec-a284-c697429f6b36",
      "metadata": {
        "id": "c3cf617f-448b-41ec-a284-c697429f6b36"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "table_images_paths = glob.glob(\"./table_images/*.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "784f1b3b-5f97-4ca1-acc0-6407e3ec663a",
      "metadata": {
        "id": "784f1b3b-5f97-4ca1-acc0-6407e3ec663a"
      },
      "outputs": [],
      "source": [
        "plot_images(table_images_paths)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}