{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2095ec2c-4927-41da-b3be-8f46e1142672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tables and rag llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184713a0-7ff1-443f-a61a-e599e6ed1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7399db6-e9d9-4599-9ba2-62040b0ca3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"llama2.pdf\"\n",
    "\n",
    "# Split the base name and extension\n",
    "output_directory_path, _ = os.path.splitext(pdf_file)\n",
    "\n",
    "if not os.path.exists(output_directory_path):\n",
    "    os.makedirs(output_directory_path)\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_document = fitz.open(pdf_file)\n",
    "\n",
    "# Iterate through each page and convert to an image\n",
    "for page_number in range(pdf_document.page_count):\n",
    "    # Get the page\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Convert the page to an image\n",
    "    pix = page.get_pixmap()\n",
    "\n",
    "    # Create a Pillow Image object from the pixmap\n",
    "    image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "    # Save the image\n",
    "    image.save(f\"./{output_directory_path}/page_{page_number + 1}.png\")\n",
    "\n",
    "# Close the PDF file\n",
    "pdf_document.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d7f70f-f81d-4e0f-bbe2-b47a5cf6b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://docs.llamaindex.ai/en/v0.9.48/examples/multi_modal/multi_modal_pdf_tables.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a5030b-2e4c-4d8a-9a8c-b730416fe46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index qdrant_client pyMuPDF tools frontend git+https://github.com/openai/CLIP.git easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c0a2e-ffd5-409d-ae79-f473db3a6430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Patch\n",
    "import io\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import AutoModelForObjectDetection\n",
    "import torch\n",
    "import openai\n",
    "import os\n",
    "import fitz\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "OPENAI_API_TOKEN = \"sk-<your-openai-api-token>\"\n",
    "openai.api_key = OPENAI_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b38eb7-eac9-44b5-876a-aebb83d9bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxResize(object):\n",
    "    def __init__(self, max_size=800):\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        width, height = image.size\n",
    "        current_max_size = max(width, height)\n",
    "        scale = self.max_size / current_max_size\n",
    "        resized_image = image.resize(\n",
    "            (int(round(scale * width)), int(round(scale * height)))\n",
    "        )\n",
    "\n",
    "        return resized_image\n",
    "\n",
    "\n",
    "detection_transform = transforms.Compose(\n",
    "    [\n",
    "        MaxResize(800),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "structure_transform = transforms.Compose(\n",
    "    [\n",
    "        MaxResize(1000),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load table detection model\n",
    "# processor = TableTransformerImageProcessor(max_size=800)\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    \"microsoft/table-transformer-detection\", revision=\"no_timm\"\n",
    ").to(device)\n",
    "\n",
    "# load table structure recognition model\n",
    "# structure_processor = TableTransformerImageProcessor(max_size=1000)\n",
    "structure_model = AutoModelForObjectDetection.from_pretrained(\n",
    "    \"microsoft/table-transformer-structure-recognition-v1.1-all\"\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    width, height = size\n",
    "    boxes = box_cxcywh_to_xyxy(out_bbox)\n",
    "    boxes = boxes * torch.tensor(\n",
    "        [width, height, width, height], dtype=torch.float32\n",
    "    )\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def outputs_to_objects(outputs, img_size, id2label):\n",
    "    m = outputs.logits.softmax(-1).max(-1)\n",
    "    pred_labels = list(m.indices.detach().cpu().numpy())[0]\n",
    "    pred_scores = list(m.values.detach().cpu().numpy())[0]\n",
    "    pred_bboxes = outputs[\"pred_boxes\"].detach().cpu()[0]\n",
    "    pred_bboxes = [\n",
    "        elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)\n",
    "    ]\n",
    "\n",
    "    objects = []\n",
    "    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):\n",
    "        class_label = id2label[int(label)]\n",
    "        if not class_label == \"no object\":\n",
    "            objects.append(\n",
    "                {\n",
    "                    \"label\": class_label,\n",
    "                    \"score\": float(score),\n",
    "                    \"bbox\": [float(elem) for elem in bbox],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return objects\n",
    "\n",
    "def detect_and_crop_save_table(\n",
    "    file_path, cropped_table_directory=\"./table_images/\"\n",
    "):\n",
    "    image = Image.open(file_path)\n",
    "\n",
    "    filename, _ = os.path.splitext(file_path.split(\"/\")[-1])\n",
    "\n",
    "    if not os.path.exists(cropped_table_directory):\n",
    "        os.makedirs(cropped_table_directory)\n",
    "\n",
    "    # prepare image for the model\n",
    "    # pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = detection_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "\n",
    "    # postprocess to get detected tables\n",
    "    id2label = model.config.id2label\n",
    "    id2label[len(model.config.id2label)] = \"no object\"\n",
    "    detected_tables = outputs_to_objects(outputs, image.size, id2label)\n",
    "\n",
    "    print(f\"number of tables detected {len(detected_tables)}\")\n",
    "\n",
    "    for idx in range(len(detected_tables)):\n",
    "        #   # crop detected table out of image\n",
    "        cropped_table = image.crop(detected_tables[idx][\"bbox\"])\n",
    "        cropped_table.save(f\"./{cropped_table_directory}/{filename}_{idx}.png\")\n",
    "\n",
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        if os.path.isfile(img_path):\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(2, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 9:\n",
    "                break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a090fe8-a01f-4303-a78d-8d5847f53e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in retrieved_images:\n",
    "    detect_and_crop_save_table(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62773545-2acc-4c6f-84b0-753782f2ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the cropped tables\n",
    "image_documents = SimpleDirectoryReader(\"./table_images/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25fe2e1-af75-43c7-9ee3-2dd4964c71cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "# put your local directore here\n",
    "#documents_images_v2 = SimpleDirectoryReader(\"./llama2/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa91b8d-f8b6-437f-bf9b-4b7cd15162cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_mm_llm = OpenAIMultiModal(\n",
    "    model=\"gpt-4-vision-preview\", api_key=OPENAI_API_TOKEN, max_new_tokens=1500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468cff40-d06a-42e5-ba69-4810e30513f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai_mm_llm.complete(\n",
    "    prompt=\"Compare llama2 with llama1?\",\n",
    "    image_documents=image_documents,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf617f-448b-41ec-a284-c697429f6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "table_images_paths = glob.glob(\"./table_images/*.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f1b3b-5f97-4ca1-acc0-6407e3ec663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(table_images_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
